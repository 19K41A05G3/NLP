{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19K41A05G3/NLP/blob/main/AESNLP(PR).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SydZolqazy_B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va5nMSO_0_Wu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "74a560f0-e99e-4ce8-c0c4-ef5ce6637d48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      iD                                           Response  Reviewer-1  \\\n",
              "0      1  An operating system (OS) is system software th...           4   \n",
              "1      1  An operating system is the most important soft...           5   \n",
              "2      1  Collection of programs that manages hardware r...           2   \n",
              "3      1      It is an interface user and machine(hardware)           2   \n",
              "4      1  An operating system is a software which acts a...           3   \n",
              "...   ..                                                ...         ...   \n",
              "2385   5  Single processor contains only one processer.w...           2   \n",
              "2386   5  Single processor systems are less reliable tha...           3   \n",
              "2387   5  Single processor system contains only one proc...           2   \n",
              "2388   5  Single processor can assign only one task but ...           1   \n",
              "2389   5  Single Processor\\n-> Uses a single cpu\\n-> Eas...           2   \n",
              "\n",
              "      Reviewer-2  word choice  Organization  \n",
              "0              4          3.0           1.0  \n",
              "1              5          2.0           3.0  \n",
              "2              1          1.0           1.0  \n",
              "3              1          1.0           0.0  \n",
              "4              2          2.0           1.0  \n",
              "...          ...          ...           ...  \n",
              "2385           2          NaN           NaN  \n",
              "2386           3          NaN           NaN  \n",
              "2387           2          NaN           NaN  \n",
              "2388           1          NaN           NaN  \n",
              "2389           2          NaN           NaN  \n",
              "\n",
              "[2390 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe57ff76-a696-4d21-8451-ac2cde2fa5f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iD</th>\n",
              "      <th>Response</th>\n",
              "      <th>Reviewer-1</th>\n",
              "      <th>Reviewer-2</th>\n",
              "      <th>word choice</th>\n",
              "      <th>Organization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>An operating system (OS) is system software th...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>An operating system is the most important soft...</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Collection of programs that manages hardware r...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>It is an interface user and machine(hardware)</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>An operating system is a software which acts a...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2385</th>\n",
              "      <td>5</td>\n",
              "      <td>Single processor contains only one processer.w...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2386</th>\n",
              "      <td>5</td>\n",
              "      <td>Single processor systems are less reliable tha...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2387</th>\n",
              "      <td>5</td>\n",
              "      <td>Single processor system contains only one proc...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2388</th>\n",
              "      <td>5</td>\n",
              "      <td>Single processor can assign only one task but ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>5</td>\n",
              "      <td>Single Processor\\n-&gt; Uses a single cpu\\n-&gt; Eas...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2390 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe57ff76-a696-4d21-8451-ac2cde2fa5f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fe57ff76-a696-4d21-8451-ac2cde2fa5f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fe57ff76-a696-4d21-8451-ac2cde2fa5f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "X = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/OSNLP.csv',encoding=\"ISO-8859-1\")\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOSAtI162KY9",
        "outputId": "b33a036b-e067-4328-9a77-7503035489b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       4\n",
            "1       5\n",
            "2       2\n",
            "3       2\n",
            "4       3\n",
            "       ..\n",
            "2385    2\n",
            "2386    3\n",
            "2387    2\n",
            "2388    1\n",
            "2389    2\n",
            "Name: Reviewer-1, Length: 2390, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "X = X.dropna(axis=1)\n",
        "y = X['Reviewer-1']\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw-9pWAo3bTh"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Define the model.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(300,dropout=0.5,recurrent_dropout=0.4, input_shape=[1, 2944], return_sequences=True))\n",
        "    #model.add(LSTM(300, dropout=0.5, recurrent_dropout=0.4, return_sequences=True))\n",
        "    model.add(LSTM(200, dropout=0.5, recurrent_dropout=0.4, return_sequences=True))\n",
        "    model.add(LSTM(100, return_sequences=True))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "    model.compile(loss='mean_squared_error', optimizer=tf.optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
        "    #model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvHVYPGE3hNY"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "use = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
        "aa=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiFHOR-n3n00",
        "outputId": "f9100dfc-f95b-49b6-c76e-65fa0580f8b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ubaq2EU3xAX"
      },
      "outputs": [],
      "source": [
        "def essay_to_sentences(essay_v, remove_stopwords):\n",
        "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    #tokennizer=PunktSentenceTokenizer(english.pickle)\n",
        "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append((raw_sentence))\n",
        "    #print(sentences)\n",
        "    return sentences\n",
        "\n",
        "def makeFeatureVec(words, num_features,n):\n",
        "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
        "    featureVec = np.zeros((n,num_features),dtype=\"float32\")\n",
        "    #success\n",
        "    x=use(words) #length of feature vector is (16,512) and length of use is (16,128)\n",
        "    y=len(x[0])\n",
        "#     featureVec = np.zeros((n,num_features),dtype=\"float32\")\n",
        "    featureVec = np.zeros((n,y),dtype=\"float32\")\n",
        "    featureVec = np.add(featureVec,x).flatten() # getting error while adding vectors\n",
        "    ss=num_features-y\n",
        "#     print(\"diff=\",ss)\n",
        "    featureVec = np.pad(featureVec, (0,ss), 'constant', constant_values=(0, 0))\n",
        "#     print(\"AFTER PADDING\",featureVec,len(featureVec))\n",
        "#     print(\"SHAPE\",featureVec.shape)     \n",
        "    #featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec\n",
        "\n",
        "def getAvgFeatureVecs(essays, num_features,aa):\n",
        "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features*23),dtype=\"float32\") # creating max length feature vector having 0s\n",
        "#     print(num_features*96)\n",
        "    for essay in essays:\n",
        "        \n",
        "#         #essayFeatureVecs.insert(counter,makeFeatureVec(essay,num_features,aa[counter]))\n",
        "        arr=makeFeatureVec(essay, num_features,aa[counter]).flatten() # getting error here\n",
        "#         print(arr)\n",
        "        ss=(num_features*23)-len(arr)\n",
        "        arr1=np.pad(arr,(0,ss),'constant', constant_values=(0, 0))\n",
        "#         #print(np.add(arr1,np.zeros(num_features*np.max(aa))))\n",
        "        essayFeatureVecs[counter] = np.add(arr1,np.zeros((num_features*23),dtype=\"float\"))\n",
        "#         #makeFeatureVec(essay, num_features,aa[counter]).flatten()\n",
        "#         #print(essayFeatureVecs[0])\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnnTd7b9O3zG",
        "outputId": "4cff0d53-c9ff-4860-fbff-d0a403843d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1.],\n",
              "       [5., 5., 5.],\n",
              "       [2., 2., 2.],\n",
              "       ...,\n",
              "       [2., 2., 2.],\n",
              "       [2., 2., 2.],\n",
              "       [2., 2., 2.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-6-TjSM33GG",
        "outputId": "826a216b-adbf-46e3-a090-65e9db40aadb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------Fold 1--------\n",
            "\n",
            "23\n",
            "[ 3  1  3  1  1  3  1  3  1  4  1  5  1  2  1  1  2  1  1  2  4  1  2  2\n",
            "  1  1  1  3  1  1  1  2  4  3  1  1  4  1  1  1  1  1  1  1  2  2  6  1\n",
            "  4  4  2  2  1  1  1  1  1  2  1  1  2  2  1  1  2  1  1  1  1  1  1  5\n",
            "  1  6  1  1  3  1  1  1  2  2  1  1  4  1  1  2  1  1  1  1  1  2  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  3  1  1  2  2  2  1  1  8 10  1  3  7  5\n",
            "  1  2  1  1  1  2  1  7  3  1  1  6  1  2  1  7  1  5  8  1  1  1  5  6\n",
            "  1 13  1  1  1  7 10  2  1  1  9  1  1  3  3  6  3  3  1  1  2  2  1  6\n",
            "  1  2  1  1  1  1  1  1  6  1  7  1  1  1  1  3  2  3  2  1  1  1  1  1\n",
            "  1  2  7  1  3  1  5  1  1  3  1  1  5  2  1  6  2  1  6  2  1  1  2  5\n",
            "  1  2  1  1  5  1  4  1  1  3  1  2  1  1  2  1  2  1  1  1  1  1  1  1\n",
            "  2  2  2  2  1  2  2  2  1  1  1  2  1  1 16  2  1  1  2  4  4  7  4  1\n",
            "  7  2  1  8  4  3  1  2  1  1  7  3  4  1  1  1  1  4  1  1  6  9  9  1\n",
            "  2  2  2  7  3  1  2  1  9  1  8  1  2  2  1  1  1  2  2  4  3  3  2  1\n",
            "  2  1  1  1  1  1  1  2  4  1  1  5  1  3  1  4  2  2  4  1  4  4  2  1\n",
            "  1 12  1  1  2  1  1  2  2  1  1  1  2  1  1  1  2  1  3  2  2  4  1  1\n",
            "  1  1  1  1  2  1  8  1  1  1  2  2  1  2  2  1  3 14  7  2  1  5  2  3\n",
            "  1  1  5  3 15  6  7  3  1  1  1  1  1 12  1  1  1  2  1  2 14  4  1  1\n",
            "  5  2  1  3  5  5  3  5  1  3  2  1  1 10  1  1  8  1  1  1  6  1 16  3\n",
            "  2 13  1  1  2  1  2  1  2  1  1  1  1  1  2  3  2  1  3  1  4  1  2  1\n",
            "  2  2  1  2  1  2  3  1 14  1  1  1  2  1  1  1  3  2  2  2  1  3]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 1, 300)            3894000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1, 200)            400800    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 1, 100)            120400    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 64)                42240     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,457,505\n",
            "Trainable params: 4,457,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "96/96 [==============================] - 18s 74ms/step - loss: 1.8143 - accuracy: 0.2413 - val_loss: 0.6428 - val_accuracy: 0.2768\n",
            "Epoch 2/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.7632 - accuracy: 0.2773 - val_loss: 0.5796 - val_accuracy: 0.2768\n",
            "Epoch 3/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.6809 - accuracy: 0.2753 - val_loss: 0.5884 - val_accuracy: 0.2768\n",
            "Epoch 4/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6516 - accuracy: 0.2760 - val_loss: 0.5542 - val_accuracy: 0.2768\n",
            "Epoch 5/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6649 - accuracy: 0.2786 - val_loss: 0.5441 - val_accuracy: 0.2768\n",
            "Epoch 6/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6204 - accuracy: 0.2753 - val_loss: 0.5718 - val_accuracy: 0.2768\n",
            "Epoch 7/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6366 - accuracy: 0.2773 - val_loss: 0.5594 - val_accuracy: 0.2768\n",
            "Epoch 8/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6224 - accuracy: 0.2773 - val_loss: 0.5534 - val_accuracy: 0.2768\n",
            "Epoch 9/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6208 - accuracy: 0.2780 - val_loss: 0.5547 - val_accuracy: 0.2768\n",
            "Epoch 10/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5851 - accuracy: 0.2780 - val_loss: 0.5888 - val_accuracy: 0.2768\n",
            "Epoch 11/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5800 - accuracy: 0.2780 - val_loss: 0.6046 - val_accuracy: 0.2768\n",
            "Epoch 12/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6090 - accuracy: 0.2786 - val_loss: 0.5442 - val_accuracy: 0.2768\n",
            "Epoch 13/50\n",
            "96/96 [==============================] - 9s 90ms/step - loss: 0.5927 - accuracy: 0.2799 - val_loss: 0.5607 - val_accuracy: 0.2768\n",
            "Epoch 14/50\n",
            "96/96 [==============================] - 6s 63ms/step - loss: 0.5623 - accuracy: 0.2786 - val_loss: 0.5794 - val_accuracy: 0.2768\n",
            "Epoch 15/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5603 - accuracy: 0.2786 - val_loss: 0.5995 - val_accuracy: 0.2768\n",
            "Epoch 16/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5597 - accuracy: 0.2825 - val_loss: 0.5465 - val_accuracy: 0.2768\n",
            "Epoch 17/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5320 - accuracy: 0.2793 - val_loss: 0.5755 - val_accuracy: 0.2794\n",
            "Epoch 18/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5114 - accuracy: 0.2812 - val_loss: 0.5592 - val_accuracy: 0.2768\n",
            "Epoch 19/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5266 - accuracy: 0.2838 - val_loss: 0.5567 - val_accuracy: 0.2768\n",
            "Epoch 20/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5257 - accuracy: 0.2793 - val_loss: 0.5740 - val_accuracy: 0.2768\n",
            "Epoch 21/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5015 - accuracy: 0.2825 - val_loss: 0.6106 - val_accuracy: 0.2846\n",
            "Epoch 22/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4935 - accuracy: 0.2799 - val_loss: 0.5578 - val_accuracy: 0.2846\n",
            "Epoch 23/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4950 - accuracy: 0.2780 - val_loss: 0.5572 - val_accuracy: 0.2846\n",
            "Epoch 24/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5120 - accuracy: 0.2786 - val_loss: 0.5534 - val_accuracy: 0.2794\n",
            "Epoch 25/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4956 - accuracy: 0.2825 - val_loss: 0.5412 - val_accuracy: 0.2794\n",
            "Epoch 26/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4858 - accuracy: 0.2832 - val_loss: 0.5600 - val_accuracy: 0.2794\n",
            "Epoch 27/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4949 - accuracy: 0.2812 - val_loss: 0.5879 - val_accuracy: 0.2794\n",
            "Epoch 28/50\n",
            "96/96 [==============================] - 7s 77ms/step - loss: 0.4734 - accuracy: 0.2819 - val_loss: 0.5718 - val_accuracy: 0.2846\n",
            "Epoch 29/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4515 - accuracy: 0.2786 - val_loss: 0.5513 - val_accuracy: 0.2846\n",
            "Epoch 30/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4924 - accuracy: 0.2825 - val_loss: 0.5414 - val_accuracy: 0.2872\n",
            "Epoch 31/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4751 - accuracy: 0.2865 - val_loss: 0.5550 - val_accuracy: 0.2846\n",
            "Epoch 32/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4643 - accuracy: 0.2838 - val_loss: 0.5492 - val_accuracy: 0.2794\n",
            "Epoch 33/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4325 - accuracy: 0.2871 - val_loss: 0.5274 - val_accuracy: 0.2794\n",
            "Epoch 34/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4634 - accuracy: 0.2838 - val_loss: 0.5394 - val_accuracy: 0.2794\n",
            "Epoch 35/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4497 - accuracy: 0.2825 - val_loss: 0.5585 - val_accuracy: 0.2794\n",
            "Epoch 36/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4738 - accuracy: 0.2825 - val_loss: 0.5694 - val_accuracy: 0.2846\n",
            "Epoch 37/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4702 - accuracy: 0.2832 - val_loss: 0.5366 - val_accuracy: 0.2820\n",
            "Epoch 38/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4449 - accuracy: 0.2832 - val_loss: 0.5321 - val_accuracy: 0.2820\n",
            "Epoch 39/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4419 - accuracy: 0.2858 - val_loss: 0.5492 - val_accuracy: 0.2846\n",
            "Epoch 40/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4367 - accuracy: 0.2865 - val_loss: 0.5876 - val_accuracy: 0.2846\n",
            "Epoch 41/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4424 - accuracy: 0.2825 - val_loss: 0.5417 - val_accuracy: 0.2846\n",
            "Epoch 42/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4396 - accuracy: 0.2858 - val_loss: 0.5511 - val_accuracy: 0.2872\n",
            "Epoch 43/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4428 - accuracy: 0.2838 - val_loss: 0.5575 - val_accuracy: 0.2846\n",
            "Epoch 44/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4171 - accuracy: 0.2865 - val_loss: 0.5446 - val_accuracy: 0.2846\n",
            "Epoch 45/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4151 - accuracy: 0.2865 - val_loss: 0.5482 - val_accuracy: 0.2820\n",
            "Epoch 46/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4221 - accuracy: 0.2871 - val_loss: 0.5528 - val_accuracy: 0.2846\n",
            "Epoch 47/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4133 - accuracy: 0.2858 - val_loss: 0.5305 - val_accuracy: 0.2846\n",
            "Epoch 48/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4252 - accuracy: 0.2819 - val_loss: 0.5323 - val_accuracy: 0.2872\n",
            "Epoch 49/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.3942 - accuracy: 0.2871 - val_loss: 0.5194 - val_accuracy: 0.2846\n",
            "Epoch 50/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4067 - accuracy: 0.2865 - val_loss: 0.5397 - val_accuracy: 0.2846\n",
            "15/15 [==============================] - 1s 18ms/step\n",
            "Kappa Score: 0.6647712956274882\n",
            "\n",
            "--------Fold 2--------\n",
            "\n",
            "23\n",
            "[ 3  4  3  2  1  1  3  1  1  1  1  1  1  1  3  2  1  1  1  1  2  1  5  1\n",
            "  1  1  1  1  1  1  1  2  1  2  1  1  2  1  3  1  1  1  1  1  1  1  1  1\n",
            "  3  2  1  2  1  3  1  1  1  1  2  1  1  1  1  2  1  1  2  1  1  1  1  1\n",
            "  1  1  1  1  1  1  2  3  2  2  1  1  4  2  1  1  1  1  2  1  1  1  2  1\n",
            "  1  3  2  1  1  1  1  2  1  2  1 10  1  7  9  7  1 13  1  1  7  1  1  7\n",
            "  1  1  2  1  2  1  1  1  1  1  8  5  1  1 10  1  3  6  6  4  1  1  1  4\n",
            "  4  3  1  7  2  1  2  2 10  2  8  9  1  7  5  1  1  1  8  5  1  4  3  6\n",
            "  4  7  1  4  1  8  1  8  1  7  1  5  1  6  1  1  4  1  2  1  3  1  1  1\n",
            "  1  1  1  2  1  1 10  1  1  1  8  1  1  6  6  1  1  2  3  1  3  7  1  1\n",
            "  1  1  1  1  2  2  1  1  1  6  1  5  5  1  2  3  7  5  2  1  1  1  1  3\n",
            "  1  1  1  3  1 15  1  1  1  3  1  2  1  1  2  1  2  2  1  1  1  1  1  1\n",
            "  1  2  1  2 10  1  1  1  1  1  1  2  2  1  2  1  1  2  6  2  7  1  1  1\n",
            "  1  3  6  1  1  1  1  1  1  1  1  2  3  3  1  2  1  3  1  3  2  1  1  1\n",
            "  1  1  1  1  2  1  2 11  1  1  1  1  1  1  1  1  1  1  1  2  2  3  4  4\n",
            "  1  1  3  4  2  1  2  2  2  1  1  1  4  1  6  1  1  2  1  1  1  1  1  1\n",
            "  1  1  3  1  1  2  1  1  1  1  2  1  2  1  2  1  1  1  2  4  1  1  1  1\n",
            "  1  7  1  5  3 11  3  1  1  1  1  1  3  3  1  1  1  2  2  5  2  3  3  1\n",
            "  2  5  2  1  5  3  7  2  5  1  1  1  1  3 14  6  6  3  5  1  6  5  1  3\n",
            "  1  1  1  3  1  1  5  1  1  3  1  1  3  1  1  1  6  1  1  1 15  2  2  4\n",
            "  1  1  3  2  1  1  1  3  9  1  1  1  2 14 12  2  1  1  2  1  1  1]\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 1, 300)            3894000   \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 1, 200)            400800    \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 1, 100)            120400    \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 64)                42240     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,457,505\n",
            "Trainable params: 4,457,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96/96 [==============================] - 17s 74ms/step - loss: 1.9295 - accuracy: 0.2315 - val_loss: 0.6966 - val_accuracy: 0.2715\n",
            "Epoch 2/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.7768 - accuracy: 0.2681 - val_loss: 0.6180 - val_accuracy: 0.2715\n",
            "Epoch 3/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.7679 - accuracy: 0.2714 - val_loss: 0.6440 - val_accuracy: 0.2715\n",
            "Epoch 4/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6933 - accuracy: 0.2727 - val_loss: 0.5442 - val_accuracy: 0.2715\n",
            "Epoch 5/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6934 - accuracy: 0.2721 - val_loss: 0.5528 - val_accuracy: 0.2715\n",
            "Epoch 6/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.6964 - accuracy: 0.2695 - val_loss: 0.6513 - val_accuracy: 0.2715\n",
            "Epoch 7/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6631 - accuracy: 0.2727 - val_loss: 0.5614 - val_accuracy: 0.2715\n",
            "Epoch 8/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6517 - accuracy: 0.2753 - val_loss: 0.5702 - val_accuracy: 0.2715\n",
            "Epoch 9/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.6580 - accuracy: 0.2721 - val_loss: 0.5681 - val_accuracy: 0.2715\n",
            "Epoch 10/50\n",
            "96/96 [==============================] - 7s 74ms/step - loss: 0.6511 - accuracy: 0.2727 - val_loss: 0.5630 - val_accuracy: 0.2715\n",
            "Epoch 11/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6011 - accuracy: 0.2760 - val_loss: 0.6188 - val_accuracy: 0.2715\n",
            "Epoch 12/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5840 - accuracy: 0.2773 - val_loss: 0.5337 - val_accuracy: 0.2715\n",
            "Epoch 13/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5865 - accuracy: 0.2760 - val_loss: 0.5567 - val_accuracy: 0.2715\n",
            "Epoch 14/50\n",
            "96/96 [==============================] - 6s 59ms/step - loss: 0.5986 - accuracy: 0.2747 - val_loss: 0.5886 - val_accuracy: 0.2715\n",
            "Epoch 15/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5946 - accuracy: 0.2767 - val_loss: 0.5594 - val_accuracy: 0.2715\n",
            "Epoch 16/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.5880 - accuracy: 0.2721 - val_loss: 0.6398 - val_accuracy: 0.2715\n",
            "Epoch 17/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.5860 - accuracy: 0.2786 - val_loss: 0.5855 - val_accuracy: 0.2715\n",
            "Epoch 18/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5566 - accuracy: 0.2767 - val_loss: 0.6283 - val_accuracy: 0.2715\n",
            "Epoch 19/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.5456 - accuracy: 0.2727 - val_loss: 0.5264 - val_accuracy: 0.2715\n",
            "Epoch 20/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5212 - accuracy: 0.2780 - val_loss: 0.5534 - val_accuracy: 0.2715\n",
            "Epoch 21/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5391 - accuracy: 0.2767 - val_loss: 0.5411 - val_accuracy: 0.2742\n",
            "Epoch 22/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5336 - accuracy: 0.2819 - val_loss: 0.5489 - val_accuracy: 0.2742\n",
            "Epoch 23/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5261 - accuracy: 0.2806 - val_loss: 0.5405 - val_accuracy: 0.2715\n",
            "Epoch 24/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5165 - accuracy: 0.2753 - val_loss: 0.5940 - val_accuracy: 0.2715\n",
            "Epoch 25/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5256 - accuracy: 0.2773 - val_loss: 0.5363 - val_accuracy: 0.2715\n",
            "Epoch 26/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5110 - accuracy: 0.2753 - val_loss: 0.5364 - val_accuracy: 0.2689\n",
            "Epoch 27/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5365 - accuracy: 0.2786 - val_loss: 0.5712 - val_accuracy: 0.2768\n",
            "Epoch 28/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5204 - accuracy: 0.2786 - val_loss: 0.5566 - val_accuracy: 0.2689\n",
            "Epoch 29/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5033 - accuracy: 0.2806 - val_loss: 0.5380 - val_accuracy: 0.2768\n",
            "Epoch 30/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5135 - accuracy: 0.2773 - val_loss: 0.5399 - val_accuracy: 0.2768\n",
            "Epoch 31/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5177 - accuracy: 0.2734 - val_loss: 0.5697 - val_accuracy: 0.2768\n",
            "Epoch 32/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4888 - accuracy: 0.2780 - val_loss: 0.5792 - val_accuracy: 0.2768\n",
            "Epoch 33/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5122 - accuracy: 0.2786 - val_loss: 0.5553 - val_accuracy: 0.2794\n",
            "Epoch 34/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5080 - accuracy: 0.2773 - val_loss: 0.5480 - val_accuracy: 0.2689\n",
            "Epoch 35/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5041 - accuracy: 0.2793 - val_loss: 0.5246 - val_accuracy: 0.2768\n",
            "Epoch 36/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4573 - accuracy: 0.2832 - val_loss: 0.5327 - val_accuracy: 0.2768\n",
            "Epoch 37/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4753 - accuracy: 0.2793 - val_loss: 0.5692 - val_accuracy: 0.2768\n",
            "Epoch 38/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4538 - accuracy: 0.2838 - val_loss: 0.5481 - val_accuracy: 0.2768\n",
            "Epoch 39/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4698 - accuracy: 0.2819 - val_loss: 0.5698 - val_accuracy: 0.2768\n",
            "Epoch 40/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4571 - accuracy: 0.2825 - val_loss: 0.5347 - val_accuracy: 0.2768\n",
            "Epoch 41/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4741 - accuracy: 0.2799 - val_loss: 0.5474 - val_accuracy: 0.2768\n",
            "Epoch 42/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4743 - accuracy: 0.2799 - val_loss: 0.5200 - val_accuracy: 0.2689\n",
            "Epoch 43/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4599 - accuracy: 0.2793 - val_loss: 0.5220 - val_accuracy: 0.2689\n",
            "Epoch 44/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4523 - accuracy: 0.2832 - val_loss: 0.5312 - val_accuracy: 0.2768\n",
            "Epoch 45/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4522 - accuracy: 0.2780 - val_loss: 0.5249 - val_accuracy: 0.2794\n",
            "Epoch 46/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4602 - accuracy: 0.2832 - val_loss: 0.5441 - val_accuracy: 0.2768\n",
            "Epoch 47/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4615 - accuracy: 0.2793 - val_loss: 0.5336 - val_accuracy: 0.2768\n",
            "Epoch 48/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4412 - accuracy: 0.2845 - val_loss: 0.5530 - val_accuracy: 0.2794\n",
            "Epoch 49/50\n",
            "96/96 [==============================] - 7s 77ms/step - loss: 0.4222 - accuracy: 0.2865 - val_loss: 0.5800 - val_accuracy: 0.2794\n",
            "Epoch 50/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4447 - accuracy: 0.2793 - val_loss: 0.5325 - val_accuracy: 0.2768\n",
            "15/15 [==============================] - 1s 20ms/step\n",
            "Kappa Score: 0.7248476679864095\n",
            "\n",
            "--------Fold 3--------\n",
            "\n",
            "23\n",
            "[ 4  3  1  1  4  2  1 10  1  1  1  2  5  2  1  2  2  2  3  2  1  3  1  2\n",
            "  1  3  1  2  1  4  1  1  1  2  1  2  3  1  1  2  2  1  1  1  2  2  2  1\n",
            "  1  1  5  2  2  1  3  1  1  1  1  1  2  1  2  3  1  1  1  1  4  3  1  4\n",
            "  3  1  7  2  1  3  5  2  2  1  1  1  1  1  4  1  1  1  1  3  1  1  4  7\n",
            " 12  7  1  1  1  2  1  1  7  4  1  1  1  1  1  1  2  1  7  1  3  1  1  1\n",
            "  1  2  4  1  1  1  1  1 10  1  6  9  3  1  1  2  1  1  1  5  1  3  2  1\n",
            "  1  4  1  3  7  3  5  1  1  1  3  2  2  3  1  4  1  1  2  2  1  1  1  1\n",
            "  2  2  4  1  1  2  5  2  1  2  3  6  1  1  6  9  1  1  1  2  8  3  1  1\n",
            "  8  4  1  1  4  1  1  5  3  1  2  1  8  1  1  1  1  3  1  1  1  2  1  1\n",
            "  1  1  1  2  1  1  1  2  1  1  1  1  1  1  1  1  1 10  2  8  3  3  9  2\n",
            "  1  1  1  1  1  6  1  1  2  2  3  1  2  1  1  1  1  1  2  1  1  1  1  2\n",
            "  2  2  1  1  1  9  1  1  1  1  3  2  1  1  2  2  2  1  3  3  1  1  1  2\n",
            "  1  2  3  1  1  1  1  1  2  1  2  1  1  1  5  2  2  1  1  1  2  1  1  4\n",
            "  7  2  3  1  2  2  1  2  8  1  1  2  1  2  1  4  1  1  1  1  1  3  2  8\n",
            "  1  1  2  1  3  1  3  1  1  1  1  1  2  2  1  2  1  2  6  1  2  1  1  1\n",
            "  1  3  3  1  1  1  1  1  1  2  1  2  2  1  2  9  1  3  1  1  1  5  1  3\n",
            " 15  1 11  2 14  1  2  1  1  3  4  2  5  1  1  4  4 10  3  5  1  1  7  2\n",
            "  4  2  1  1  7  1  4  8  3  5  3  3  2  1  3  1  1  1  2  2  2  2  1  1\n",
            "  2  1  1  1  1  1  1  2  2  1  3  1  3  1  1  1  1  1  2  3  1  1  7  6\n",
            "  1  2  4  8  2  4  1  1  1  1  1  1  1  1  2  2  1  2  2  2  7  1]\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_8 (LSTM)               (None, 1, 300)            3894000   \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 1, 200)            400800    \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 1, 100)            120400    \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 64)                42240     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,457,505\n",
            "Trainable params: 4,457,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96/96 [==============================] - 17s 72ms/step - loss: 1.9493 - accuracy: 0.2374 - val_loss: 0.7073 - val_accuracy: 0.2428\n",
            "Epoch 2/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.7834 - accuracy: 0.2865 - val_loss: 0.6872 - val_accuracy: 0.2428\n",
            "Epoch 3/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6896 - accuracy: 0.2858 - val_loss: 0.6605 - val_accuracy: 0.2428\n",
            "Epoch 4/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6991 - accuracy: 0.2871 - val_loss: 0.6499 - val_accuracy: 0.2428\n",
            "Epoch 5/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6688 - accuracy: 0.2884 - val_loss: 0.6927 - val_accuracy: 0.2428\n",
            "Epoch 6/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6596 - accuracy: 0.2858 - val_loss: 0.6619 - val_accuracy: 0.2428\n",
            "Epoch 7/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6246 - accuracy: 0.2884 - val_loss: 0.6940 - val_accuracy: 0.2428\n",
            "Epoch 8/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6099 - accuracy: 0.2891 - val_loss: 0.7205 - val_accuracy: 0.2428\n",
            "Epoch 9/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6291 - accuracy: 0.2871 - val_loss: 0.6479 - val_accuracy: 0.2428\n",
            "Epoch 10/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5906 - accuracy: 0.2884 - val_loss: 0.6699 - val_accuracy: 0.2428\n",
            "Epoch 11/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5733 - accuracy: 0.2884 - val_loss: 0.7284 - val_accuracy: 0.2428\n",
            "Epoch 12/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6010 - accuracy: 0.2910 - val_loss: 0.6620 - val_accuracy: 0.2428\n",
            "Epoch 13/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5791 - accuracy: 0.2897 - val_loss: 0.6366 - val_accuracy: 0.2428\n",
            "Epoch 14/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5497 - accuracy: 0.2884 - val_loss: 0.7826 - val_accuracy: 0.2428\n",
            "Epoch 15/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5611 - accuracy: 0.2917 - val_loss: 0.6376 - val_accuracy: 0.2454\n",
            "Epoch 16/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5487 - accuracy: 0.2917 - val_loss: 0.6457 - val_accuracy: 0.2454\n",
            "Epoch 17/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5328 - accuracy: 0.2937 - val_loss: 0.6334 - val_accuracy: 0.2402\n",
            "Epoch 18/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5511 - accuracy: 0.2891 - val_loss: 0.6390 - val_accuracy: 0.2402\n",
            "Epoch 19/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5017 - accuracy: 0.2891 - val_loss: 0.7066 - val_accuracy: 0.2402\n",
            "Epoch 20/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5321 - accuracy: 0.2910 - val_loss: 0.6341 - val_accuracy: 0.2402\n",
            "Epoch 21/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5366 - accuracy: 0.2917 - val_loss: 0.6619 - val_accuracy: 0.2428\n",
            "Epoch 22/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5081 - accuracy: 0.2943 - val_loss: 0.6541 - val_accuracy: 0.2324\n",
            "Epoch 23/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4991 - accuracy: 0.2917 - val_loss: 0.6368 - val_accuracy: 0.2376\n",
            "Epoch 24/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4881 - accuracy: 0.2923 - val_loss: 0.7026 - val_accuracy: 0.2324\n",
            "Epoch 25/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5077 - accuracy: 0.2923 - val_loss: 0.6452 - val_accuracy: 0.2376\n",
            "Epoch 26/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4994 - accuracy: 0.2904 - val_loss: 0.6436 - val_accuracy: 0.2376\n",
            "Epoch 27/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5032 - accuracy: 0.2930 - val_loss: 0.6296 - val_accuracy: 0.2376\n",
            "Epoch 28/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4887 - accuracy: 0.2950 - val_loss: 0.6227 - val_accuracy: 0.2376\n",
            "Epoch 29/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4947 - accuracy: 0.2891 - val_loss: 0.6402 - val_accuracy: 0.2402\n",
            "Epoch 30/50\n",
            "96/96 [==============================] - 7s 69ms/step - loss: 0.4964 - accuracy: 0.2930 - val_loss: 0.6227 - val_accuracy: 0.2402\n",
            "Epoch 31/50\n",
            "96/96 [==============================] - 6s 66ms/step - loss: 0.5004 - accuracy: 0.2910 - val_loss: 0.6346 - val_accuracy: 0.2298\n",
            "Epoch 32/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5029 - accuracy: 0.2930 - val_loss: 0.7136 - val_accuracy: 0.2298\n",
            "Epoch 33/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4705 - accuracy: 0.2937 - val_loss: 0.6468 - val_accuracy: 0.2298\n",
            "Epoch 34/50\n",
            "96/96 [==============================] - 6s 59ms/step - loss: 0.4669 - accuracy: 0.2950 - val_loss: 0.6635 - val_accuracy: 0.2298\n",
            "Epoch 35/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4683 - accuracy: 0.2923 - val_loss: 0.6345 - val_accuracy: 0.2298\n",
            "Epoch 36/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4514 - accuracy: 0.2930 - val_loss: 0.6254 - val_accuracy: 0.2376\n",
            "Epoch 37/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4654 - accuracy: 0.2930 - val_loss: 0.6468 - val_accuracy: 0.2376\n",
            "Epoch 38/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4625 - accuracy: 0.2943 - val_loss: 0.6440 - val_accuracy: 0.2324\n",
            "Epoch 39/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4548 - accuracy: 0.2930 - val_loss: 0.6759 - val_accuracy: 0.2376\n",
            "Epoch 40/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4459 - accuracy: 0.2884 - val_loss: 0.6924 - val_accuracy: 0.2298\n",
            "Epoch 41/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4473 - accuracy: 0.2917 - val_loss: 0.6456 - val_accuracy: 0.2376\n",
            "Epoch 42/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4539 - accuracy: 0.2950 - val_loss: 0.6611 - val_accuracy: 0.2324\n",
            "Epoch 43/50\n",
            "96/96 [==============================] - 6s 59ms/step - loss: 0.4248 - accuracy: 0.2969 - val_loss: 0.6438 - val_accuracy: 0.2324\n",
            "Epoch 44/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4240 - accuracy: 0.2995 - val_loss: 0.6483 - val_accuracy: 0.2402\n",
            "Epoch 45/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4380 - accuracy: 0.2884 - val_loss: 0.6465 - val_accuracy: 0.2402\n",
            "Epoch 46/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4271 - accuracy: 0.2969 - val_loss: 0.6489 - val_accuracy: 0.2402\n",
            "Epoch 47/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4254 - accuracy: 0.2963 - val_loss: 0.6462 - val_accuracy: 0.2324\n",
            "Epoch 48/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4321 - accuracy: 0.2982 - val_loss: 0.6750 - val_accuracy: 0.2324\n",
            "Epoch 49/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4220 - accuracy: 0.2930 - val_loss: 0.6691 - val_accuracy: 0.2324\n",
            "Epoch 50/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4177 - accuracy: 0.2989 - val_loss: 0.6429 - val_accuracy: 0.2402\n",
            "15/15 [==============================] - 1s 19ms/step\n",
            "Kappa Score: 0.728813280601212\n",
            "\n",
            "--------Fold 4--------\n",
            "\n",
            "23\n",
            "[ 1  1  1  1  2  2  1  1  1  1  1  1  1  1  1  1  1  1  7  1  2  2  1  1\n",
            "  1  1  1  1  1  2  1  3  1  1  1  1  3  1  1  2  1  1  1  1  2  1  2  1\n",
            "  3  4  1  4  1  1  1  1  3  1  1  1  3  2  2  1  3  1  1  1  2  5  2  1\n",
            "  2  1  1  1  1  2  1  1  1  2  1  1  2  1  3  3  1  1  1  1  1  1 10  5\n",
            "  1  1 10 10  9  4  1  1 10  5  3  2  2  2 10  5  3 10  1  2  2  3  3  1\n",
            "  1  1  1  1  2  1  1 10 10  2  1  6  1 18  1  1  1  1  1  1  3  1  1  3\n",
            "  7  7  1  1  2  1  1  6  1  1  1  2  5  1  1  1 10  4  1  1  1  1  9  1\n",
            "  2  3  1  1  1  1  1  1  1  2  2  1  4  6  1  1  1 10  6  6  3  1  1  6\n",
            " 10  1  1  1  1  2  4  4  5  8  2  7  5  4  3  4  2  1  1  5  1  1  1  1\n",
            "  1  1 15  1  1  1  2  3  1  1  2  1  2  1  2  2  1  1  1  2  7  8  1  3\n",
            "  1  1  1 16  1 12  2  2  3  1  2  2 12  1  1  1  1  1  2  3  8  4  1  2\n",
            "  7  1  2  5  1  3  3  1  1  1  1  1  2  3  1  1  1  1  3  1  1  1  1  2\n",
            "  1  1  2  2  4  1  9  3  1  1  2  4  1  3  1  1  1  1  1  1  1  1  3  1\n",
            "  2  1  1  1  1  1  6  3  7  5  5 12  1  2  1  4  2  1  2  1  1  2  5  2\n",
            "  2  3  1  2  2  1  2  1  1  2  1  1  1  2  3  1  1  1  1  1  2  2  1  1\n",
            "  3  2  1  1 10  2  1  1  1  2  1  3  1  1  1  1  1  1  1  2  3  3  1  6\n",
            "  1  1  1  2  3  1  1  1  1  2  2  3  1  2  1  1  1  2 10  5  5  3  2  1\n",
            "  1  2  3  2  1  3  1  7  5  3  1  2  3  1  2  1  2  4  4  3  2  3  9  1\n",
            "  2  1  1  1  1  1  3  7  1  3  3  1  5  3  1  1  1 14  5  2  2  3  2  2\n",
            "  4  1  2  2  4  1  2  1  1  1  5  2  2  1  2  1  1  2  4  1  3  3]\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_12 (LSTM)              (None, 1, 300)            3894000   \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, 1, 200)            400800    \n",
            "                                                                 \n",
            " lstm_14 (LSTM)              (None, 1, 100)            120400    \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              (None, 64)                42240     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,457,505\n",
            "Trainable params: 4,457,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96/96 [==============================] - 16s 73ms/step - loss: 1.8171 - accuracy: 0.2276 - val_loss: 0.7899 - val_accuracy: 0.2898\n",
            "Epoch 2/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.7419 - accuracy: 0.2727 - val_loss: 0.6970 - val_accuracy: 0.2898\n",
            "Epoch 3/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6898 - accuracy: 0.2747 - val_loss: 0.6605 - val_accuracy: 0.2898\n",
            "Epoch 4/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6885 - accuracy: 0.2767 - val_loss: 0.6674 - val_accuracy: 0.2898\n",
            "Epoch 5/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6502 - accuracy: 0.2721 - val_loss: 0.6723 - val_accuracy: 0.2898\n",
            "Epoch 6/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6690 - accuracy: 0.2773 - val_loss: 0.6503 - val_accuracy: 0.2898\n",
            "Epoch 7/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6374 - accuracy: 0.2767 - val_loss: 0.6545 - val_accuracy: 0.2898\n",
            "Epoch 8/50\n",
            "96/96 [==============================] - 6s 59ms/step - loss: 0.6273 - accuracy: 0.2721 - val_loss: 0.6318 - val_accuracy: 0.2898\n",
            "Epoch 9/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.6013 - accuracy: 0.2773 - val_loss: 0.6408 - val_accuracy: 0.2898\n",
            "Epoch 10/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5916 - accuracy: 0.2773 - val_loss: 0.6297 - val_accuracy: 0.2898\n",
            "Epoch 11/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6019 - accuracy: 0.2740 - val_loss: 0.6277 - val_accuracy: 0.2898\n",
            "Epoch 12/50\n",
            "96/96 [==============================] - 7s 77ms/step - loss: 0.5726 - accuracy: 0.2767 - val_loss: 0.6305 - val_accuracy: 0.2898\n",
            "Epoch 13/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5593 - accuracy: 0.2799 - val_loss: 0.6344 - val_accuracy: 0.2898\n",
            "Epoch 14/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5504 - accuracy: 0.2753 - val_loss: 0.6460 - val_accuracy: 0.2898\n",
            "Epoch 15/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5899 - accuracy: 0.2767 - val_loss: 0.6149 - val_accuracy: 0.2898\n",
            "Epoch 16/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5477 - accuracy: 0.2767 - val_loss: 0.6342 - val_accuracy: 0.2898\n",
            "Epoch 17/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5436 - accuracy: 0.2793 - val_loss: 0.6140 - val_accuracy: 0.2898\n",
            "Epoch 18/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5118 - accuracy: 0.2799 - val_loss: 0.6047 - val_accuracy: 0.2924\n",
            "Epoch 19/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5239 - accuracy: 0.2767 - val_loss: 0.6356 - val_accuracy: 0.2898\n",
            "Epoch 20/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5556 - accuracy: 0.2793 - val_loss: 0.5916 - val_accuracy: 0.2898\n",
            "Epoch 21/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5141 - accuracy: 0.2793 - val_loss: 0.6032 - val_accuracy: 0.2898\n",
            "Epoch 22/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5107 - accuracy: 0.2825 - val_loss: 0.6284 - val_accuracy: 0.2924\n",
            "Epoch 23/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5391 - accuracy: 0.2780 - val_loss: 0.6064 - val_accuracy: 0.2898\n",
            "Epoch 24/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5163 - accuracy: 0.2819 - val_loss: 0.6288 - val_accuracy: 0.2898\n",
            "Epoch 25/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4999 - accuracy: 0.2799 - val_loss: 0.6257 - val_accuracy: 0.2898\n",
            "Epoch 26/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5010 - accuracy: 0.2767 - val_loss: 0.6168 - val_accuracy: 0.2924\n",
            "Epoch 27/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4904 - accuracy: 0.2812 - val_loss: 0.5962 - val_accuracy: 0.2977\n",
            "Epoch 28/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4884 - accuracy: 0.2838 - val_loss: 0.5977 - val_accuracy: 0.2977\n",
            "Epoch 29/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4772 - accuracy: 0.2806 - val_loss: 0.5879 - val_accuracy: 0.2924\n",
            "Epoch 30/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4830 - accuracy: 0.2780 - val_loss: 0.6565 - val_accuracy: 0.2950\n",
            "Epoch 31/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5034 - accuracy: 0.2799 - val_loss: 0.6341 - val_accuracy: 0.2950\n",
            "Epoch 32/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4629 - accuracy: 0.2819 - val_loss: 0.6074 - val_accuracy: 0.2924\n",
            "Epoch 33/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4700 - accuracy: 0.2780 - val_loss: 0.6079 - val_accuracy: 0.2977\n",
            "Epoch 34/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4714 - accuracy: 0.2819 - val_loss: 0.5972 - val_accuracy: 0.2977\n",
            "Epoch 35/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4727 - accuracy: 0.2838 - val_loss: 0.5954 - val_accuracy: 0.2924\n",
            "Epoch 36/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4741 - accuracy: 0.2806 - val_loss: 0.5949 - val_accuracy: 0.2924\n",
            "Epoch 37/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4446 - accuracy: 0.2825 - val_loss: 0.5874 - val_accuracy: 0.2924\n",
            "Epoch 38/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4454 - accuracy: 0.2838 - val_loss: 0.5997 - val_accuracy: 0.2924\n",
            "Epoch 39/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4456 - accuracy: 0.2819 - val_loss: 0.5887 - val_accuracy: 0.2924\n",
            "Epoch 40/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4407 - accuracy: 0.2865 - val_loss: 0.5808 - val_accuracy: 0.2977\n",
            "Epoch 41/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4490 - accuracy: 0.2825 - val_loss: 0.5906 - val_accuracy: 0.2924\n",
            "Epoch 42/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4461 - accuracy: 0.2865 - val_loss: 0.6122 - val_accuracy: 0.2924\n",
            "Epoch 43/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4370 - accuracy: 0.2838 - val_loss: 0.5847 - val_accuracy: 0.2924\n",
            "Epoch 44/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4296 - accuracy: 0.2747 - val_loss: 0.5789 - val_accuracy: 0.2924\n",
            "Epoch 45/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4460 - accuracy: 0.2845 - val_loss: 0.5936 - val_accuracy: 0.2924\n",
            "Epoch 46/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4379 - accuracy: 0.2832 - val_loss: 0.5850 - val_accuracy: 0.2924\n",
            "Epoch 47/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4396 - accuracy: 0.2884 - val_loss: 0.5741 - val_accuracy: 0.2950\n",
            "Epoch 48/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4195 - accuracy: 0.2786 - val_loss: 0.5795 - val_accuracy: 0.2950\n",
            "Epoch 49/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4346 - accuracy: 0.2812 - val_loss: 0.5941 - val_accuracy: 0.2924\n",
            "Epoch 50/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4176 - accuracy: 0.2845 - val_loss: 0.5836 - val_accuracy: 0.2924\n",
            "15/15 [==============================] - 2s 19ms/step\n",
            "Kappa Score: 0.6943533920239534\n",
            "\n",
            "--------Fold 5--------\n",
            "\n",
            "18\n",
            "[ 1  1  1  1  1  3  1  1  3  1  1  1  1  1  1  3  1  1  1  1  1  3  1  2\n",
            "  1  3  1  1  2  2  1  1  1  1  1 10  1  1  1  1  1 12  4  3  3  1  3  1\n",
            "  1  6  1  2  2  1  1  4  1  1  1  1  5  1  1  1  1  1  3  5  1  1  1  1\n",
            "  1  1  1  1  1  3  1  2  4  1  1  1  1  1  1  3  4  2  1  1  3  3  1  1\n",
            "  1  1  2  2  1  1  1  1  4  1  1  1  1  2  1  1  1  2  1  5  1  1 10  1\n",
            "  9  1  1  5  7  1  1  1  1  1  9  1 10  1  1  2  1  5  1  5  3  1 10  3\n",
            "  9  9  7  1  1  9  1  1  1  1 10  1  4 10  2  3  6  3  2  1  6  1  2  2\n",
            "  7  7  2  7  7 13  7  1  1  1  1  1  1  7  1  7  1  1  1  1  3  1  1  1\n",
            "  5  2  5  1 10  1  1  1  1  1  2  2  1  4  1  5  1  1  2  1  5  4  7  1\n",
            "  2  1  1  1  1  1  1  1  1  1  1  4  1  1  5  1  1  1  2  2  2  2  1  8\n",
            "  1  1  1 17  1  1  1  1  2  1  2  6  4  1  2  3  1  1  2  2  1 12  2  1\n",
            "  2 18  2  1  1  1  4  2  2  1  2  4  1  1  3  2  1  1  2  1  1  1  1  1\n",
            "  1  1  3 16  2  3  2  1  2  9  5  3  1  4  2  1  1  3  1  8  3 16  1  6\n",
            "  3  1  3  3  1  2  2  3  2  2  2  1  3  4  1  1  3  1  3  1  2  1  2  1\n",
            "  1  3  1  1  1  2  1  2  1  1  2  2  3  1  2  1  1  1  2  1  1  1  7  1\n",
            "  2  1  2  1  1  1  2  1  1  1  2  1  1  2  2  3  1  1  1  2  2  1  2  5\n",
            "  1  3 10  2  1  1  1  1  3  1  1  1  2  2  1  1  1  1  1  1  4  9  1  1\n",
            "  1  5  1  4  1 21 23  2  3  3  5  3  5  1  5  1  3  2 11  1  3  7  1  8\n",
            "  7  5  5  3  3  3  1  2  2  6  2  6  1  2  1  4  1  3  2  3  1  1  3  1\n",
            "  2  1  2  1  1  1  5  4  8  4  2  1  2  1  1  1  1  2  1  1  1  1]\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_16 (LSTM)              (None, 1, 300)            3894000   \n",
            "                                                                 \n",
            " lstm_17 (LSTM)              (None, 1, 200)            400800    \n",
            "                                                                 \n",
            " lstm_18 (LSTM)              (None, 1, 100)            120400    \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 64)                42240     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,457,505\n",
            "Trainable params: 4,457,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96/96 [==============================] - 17s 73ms/step - loss: 1.8065 - accuracy: 0.2387 - val_loss: 0.7279 - val_accuracy: 0.2689\n",
            "Epoch 2/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.7477 - accuracy: 0.2865 - val_loss: 0.6903 - val_accuracy: 0.2689\n",
            "Epoch 3/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.7012 - accuracy: 0.2884 - val_loss: 0.6074 - val_accuracy: 0.2689\n",
            "Epoch 4/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6632 - accuracy: 0.2910 - val_loss: 0.6153 - val_accuracy: 0.2689\n",
            "Epoch 5/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6649 - accuracy: 0.2891 - val_loss: 0.6478 - val_accuracy: 0.2689\n",
            "Epoch 6/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6188 - accuracy: 0.2904 - val_loss: 0.6167 - val_accuracy: 0.2689\n",
            "Epoch 7/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.6237 - accuracy: 0.2891 - val_loss: 0.6328 - val_accuracy: 0.2689\n",
            "Epoch 8/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.5951 - accuracy: 0.2891 - val_loss: 0.6055 - val_accuracy: 0.2689\n",
            "Epoch 9/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.6155 - accuracy: 0.2865 - val_loss: 0.5843 - val_accuracy: 0.2689\n",
            "Epoch 10/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5873 - accuracy: 0.2884 - val_loss: 0.6190 - val_accuracy: 0.2689\n",
            "Epoch 11/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5630 - accuracy: 0.2923 - val_loss: 0.5983 - val_accuracy: 0.2715\n",
            "Epoch 12/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5640 - accuracy: 0.2930 - val_loss: 0.5894 - val_accuracy: 0.2689\n",
            "Epoch 13/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5497 - accuracy: 0.2891 - val_loss: 0.5752 - val_accuracy: 0.2689\n",
            "Epoch 14/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5534 - accuracy: 0.2865 - val_loss: 0.5754 - val_accuracy: 0.2689\n",
            "Epoch 15/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5228 - accuracy: 0.2910 - val_loss: 0.5725 - val_accuracy: 0.2715\n",
            "Epoch 16/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5392 - accuracy: 0.2865 - val_loss: 0.5864 - val_accuracy: 0.2715\n",
            "Epoch 17/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5348 - accuracy: 0.2917 - val_loss: 0.5715 - val_accuracy: 0.2689\n",
            "Epoch 18/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5195 - accuracy: 0.2884 - val_loss: 0.5875 - val_accuracy: 0.2689\n",
            "Epoch 19/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5134 - accuracy: 0.2917 - val_loss: 0.5743 - val_accuracy: 0.2715\n",
            "Epoch 20/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5346 - accuracy: 0.2891 - val_loss: 0.5670 - val_accuracy: 0.2742\n",
            "Epoch 21/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5193 - accuracy: 0.2891 - val_loss: 0.5794 - val_accuracy: 0.2715\n",
            "Epoch 22/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5072 - accuracy: 0.2943 - val_loss: 0.5845 - val_accuracy: 0.2715\n",
            "Epoch 23/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5045 - accuracy: 0.2917 - val_loss: 0.5871 - val_accuracy: 0.2715\n",
            "Epoch 24/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.5113 - accuracy: 0.2897 - val_loss: 0.5616 - val_accuracy: 0.2715\n",
            "Epoch 25/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.5133 - accuracy: 0.2904 - val_loss: 0.5570 - val_accuracy: 0.2715\n",
            "Epoch 26/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4967 - accuracy: 0.2878 - val_loss: 0.5834 - val_accuracy: 0.2742\n",
            "Epoch 27/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4758 - accuracy: 0.2943 - val_loss: 0.5623 - val_accuracy: 0.2742\n",
            "Epoch 28/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4736 - accuracy: 0.2910 - val_loss: 0.5462 - val_accuracy: 0.2742\n",
            "Epoch 29/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4668 - accuracy: 0.2910 - val_loss: 0.6271 - val_accuracy: 0.2715\n",
            "Epoch 30/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4710 - accuracy: 0.2963 - val_loss: 0.5670 - val_accuracy: 0.2715\n",
            "Epoch 31/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4536 - accuracy: 0.2937 - val_loss: 0.5656 - val_accuracy: 0.2742\n",
            "Epoch 32/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4699 - accuracy: 0.2910 - val_loss: 0.5729 - val_accuracy: 0.2742\n",
            "Epoch 33/50\n",
            "96/96 [==============================] - 7s 76ms/step - loss: 0.4728 - accuracy: 0.2963 - val_loss: 0.5761 - val_accuracy: 0.2715\n",
            "Epoch 34/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4476 - accuracy: 0.2917 - val_loss: 0.5662 - val_accuracy: 0.2742\n",
            "Epoch 35/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4608 - accuracy: 0.2923 - val_loss: 0.5657 - val_accuracy: 0.2742\n",
            "Epoch 36/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4459 - accuracy: 0.2917 - val_loss: 0.5552 - val_accuracy: 0.2742\n",
            "Epoch 37/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4585 - accuracy: 0.2884 - val_loss: 0.5913 - val_accuracy: 0.2742\n",
            "Epoch 38/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4439 - accuracy: 0.2910 - val_loss: 0.5587 - val_accuracy: 0.2742\n",
            "Epoch 39/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4690 - accuracy: 0.2923 - val_loss: 0.5752 - val_accuracy: 0.2742\n",
            "Epoch 40/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4630 - accuracy: 0.2917 - val_loss: 0.5700 - val_accuracy: 0.2715\n",
            "Epoch 41/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4353 - accuracy: 0.2923 - val_loss: 0.5609 - val_accuracy: 0.2742\n",
            "Epoch 42/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4185 - accuracy: 0.2923 - val_loss: 0.5547 - val_accuracy: 0.2715\n",
            "Epoch 43/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4293 - accuracy: 0.2937 - val_loss: 0.5531 - val_accuracy: 0.2715\n",
            "Epoch 44/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4335 - accuracy: 0.2976 - val_loss: 0.5606 - val_accuracy: 0.2742\n",
            "Epoch 45/50\n",
            "96/96 [==============================] - 6s 62ms/step - loss: 0.4082 - accuracy: 0.2943 - val_loss: 0.5500 - val_accuracy: 0.2715\n",
            "Epoch 46/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4193 - accuracy: 0.2950 - val_loss: 0.5482 - val_accuracy: 0.2742\n",
            "Epoch 47/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4160 - accuracy: 0.2930 - val_loss: 0.5559 - val_accuracy: 0.2715\n",
            "Epoch 48/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4332 - accuracy: 0.2917 - val_loss: 0.5817 - val_accuracy: 0.2742\n",
            "Epoch 49/50\n",
            "96/96 [==============================] - 6s 60ms/step - loss: 0.4079 - accuracy: 0.2969 - val_loss: 0.5555 - val_accuracy: 0.2742\n",
            "Epoch 50/50\n",
            "96/96 [==============================] - 6s 61ms/step - loss: 0.4286 - accuracy: 0.2937 - val_loss: 0.5474 - val_accuracy: 0.2715\n",
            "15/15 [==============================] - 1s 20ms/step\n",
            "Kappa Score: 0.6464885898607724\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import tensorflow as tf\n",
        "\n",
        "cv = KFold(n_splits = 5, shuffle = True)\n",
        "results = []\n",
        "y_pred_list = []\n",
        "\n",
        "count = 1\n",
        "for traincv, testcv in cv.split(X):\n",
        "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
        "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
        "    #print(X_train.isna())\n",
        "    train_essays = X_train['Response']\n",
        "    test_essays = X_test['Response']\n",
        "    y_train=y_train\n",
        "    y_test=y_test\n",
        "    num_features = 128\n",
        "    min_word_count = 40\n",
        "    num_workers = 4\n",
        "    context = 10\n",
        "    downsampling = 1e-3\n",
        "    clean_train_essays = []\n",
        "    clean_test_essays=[]\n",
        "    #essays=train_essays\n",
        "    sentences=[]\n",
        "    l1=[]\n",
        "    l2=[]\n",
        "    for essay in train_essays:\n",
        "      sentences += essay_to_sentences(essay, remove_stopwords = False)\n",
        "  \n",
        "      #print(sentences)\n",
        "    for essay in test_essays:\n",
        "      sentences += essay_to_sentences(essay, remove_stopwords = False)\n",
        "\n",
        "    for essay_v in train_essays:\n",
        "      l1.append(len(sent_tokenize(essay_v)))\n",
        "      clean_train_essays.append(essay_to_sentences(essay_v, remove_stopwords=True))\n",
        "      #print(clean_train_essays)\n",
        "    a=np.array(l1)\n",
        "    print(np.max(a))\n",
        "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, num_features,a)\n",
        "    for essay_v in test_essays:\n",
        "      l2.append(len(sent_tokenize(essay_v)))\n",
        "      clean_test_essays.append(essay_to_sentences(essay_v, remove_stopwords=True))\n",
        "    b=np.array(l2)\n",
        "    testDataVecs=getAvgFeatureVecs(clean_test_essays, num_features,b)\n",
        "    trainDataVecs=np.array(trainDataVecs)\n",
        "    testDataVecs = np.array(testDataVecs)\n",
        "    trainDataVecs=np.reshape(trainDataVecs,(trainDataVecs.shape[0],1,trainDataVecs.shape[1]))\n",
        "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "    print(b)\n",
        "    #print(y_train)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(trainDataVecs, y_train, test_size=0.2, random_state=3)\n",
        "    lstm_model = get_model()\n",
        "    history=lstm_model.fit(X_train, y_train, batch_size=16, epochs=50,validation_data=(X_val, y_val))\n",
        "    \n",
        "    #history=lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=30)\n",
        "    y_pred=(lstm_model.predict(testDataVecs))\n",
        "    #print(testDataVecs)\n",
        "    # Save any one of the 5 models.\n",
        "    if count == 6:\n",
        "      break\n",
        "         #lstm_model.save('./model_weights/final_lstm.h5')\n",
        "    \n",
        "    # Round y_pred to the nearest integer.\n",
        "    y_pred = np.around(y_pred)\n",
        "    #print(y_pred)\n",
        "    \n",
        "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
        "    result = cohen_kappa_score(y_test,y_pred,weights='quadratic')\n",
        "    print(\"Kappa Score: {}\".format(result))\n",
        "    results.append(result)\n",
        "    \n",
        "    count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbQBF-S24eOV",
        "outputId": "f30f37a9-ec90-47f7-aab5-ae93c7e67df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Kappa score after a 5-fold cross validation:  0.692\n"
          ]
        }
      ],
      "source": [
        "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "3UEZmG3n4h4h",
        "outputId": "16bc5d94-ce64-4e02-db1b-9b6845d7b801"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs = range(1,51)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "oRhAf0Kj4mi4",
        "outputId": "b42dc637-b85f-4fc1-b1a3-a75e2470c73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2       2\n",
            "3       2\n",
            "4       3\n",
            "5       1\n",
            "18      3\n",
            "       ..\n",
            "2371    1\n",
            "2373    2\n",
            "2376    1\n",
            "2377    1\n",
            "2382    1\n",
            "Name: Reviewer-1, Length: 478, dtype: int64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3e96d3446650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"actual score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"predicted score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medgecolors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m         plotnonfinite=plotnonfinite, **({\"data\": data} if data is not\n\u001b[0;32m-> 2816\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2817\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4389\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4391\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print(y_test)\n",
        "import matplotlib.pyplot as plt\n",
        "x=np.arange(0,62)\n",
        "plt.title(\"actual vs predicted score\")\n",
        "plt.xlabel(\"essays\")\n",
        "plt.ylabel(\"score\")\n",
        "\n",
        "plt.scatter(x,y_test.values,color='r',label=\"actual score\")\n",
        "plt.scatter(x,y_pred,color='b',label=\"predicted score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "btrajyugX99w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "1L8tawqM79_ufFjkpyYgGetcZQODYlIM0",
      "authorship_tag": "ABX9TyNZxkAi9umNNazRxOfNzVHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}